{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Describing, visualising, and transforming data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Descriptive statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pima Indians Dataset\n",
    "\n",
    "The Pima Indians dataset is used again to demonstrate descriptive statistics in this lab. To recap, this dataset describes the medical records for Pima Indians and whether or not each patient will have an onset of diabetes within five years. As such it is a classification problem, where input attributes are numeric and the output variable to be predicted is binary (0 or 1). \n",
    "\n",
    "The list below shows the eight attributes for the dataset as a reminder: \n",
    "\n",
    "1. Number of times pregnant.\n",
    "2. Plasma glucose concentration 2 hours in an oral glucose tolerance test. \n",
    "3. Diastolic blood pressure (mm Hg).\n",
    "4. Triceps skin fold thickness (mm).\n",
    "5. 2-Hour serum insulin (mu U/ml).\n",
    "6. Body mass index (BMI).\n",
    "7. Diabetes pedigree function.\n",
    "8. Age (years).\n",
    "9. Class, onset of diabetes within five years.\n",
    "\n",
    "We will load the dataset and take a closer look at the data we have at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancy_Count</th>\n",
       "      <th>Glucone_conc</th>\n",
       "      <th>Blood_pressure</th>\n",
       "      <th>Skin_thickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DPF</th>\n",
       "      <th>Age</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancy_Count  Glucone_conc  Blood_pressure  Skin_thickness  Insulin  \\\n",
       "0                6           148              72              35        0   \n",
       "1                1            85              66              29        0   \n",
       "2                8           183              64               0        0   \n",
       "3                1            89              66              23       94   \n",
       "4                0           137              40              35      168   \n",
       "\n",
       "    BMI    DPF  Age  Class  \n",
       "0  33.6  0.627   50      1  \n",
       "1  26.6  0.351   31      0  \n",
       "2  23.3  0.672   32      1  \n",
       "3  28.1  0.167   21      0  \n",
       "4  43.1  2.288   33      1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as pyplot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "filename = './data/pima-indians-diabetes.data.csv'\n",
    "header = ['Pregnancy_Count','Glucone_conc','Blood_pressure','Skin_thickness','Insulin','BMI','DPF','Age','Class']\n",
    "\n",
    "data = pd.read_csv(filename, names=header)\n",
    "print(data.shape)\n",
    "\n",
    "data.head()\n",
    "#data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descriptive statistics can give you great insight into the shape of each attribute. Often you can create more summaries than you have time to review. The `describe()` function on the `Pandas.DataFrame` object lists 8 statistical properties for each attribute. They are:\n",
    "- Count.\n",
    "- Mean.\n",
    "- Standard Deviation.\n",
    "- Minimum Value.\n",
    "- 25th Percentile.\n",
    "- 50th Percentile (Median). \n",
    "- 75th Percentile.\n",
    "- Maximum Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "OptionError",
     "evalue": "Pattern matched multiple keys",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOptionError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/iangill/Documents/Coding/DataSci/Week2/Descriptive statistics.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/iangill/Documents/Coding/DataSci/Week2/Descriptive%20statistics.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m pd\u001b[39m.\u001b[39mset_option(\u001b[39m'\u001b[39m\u001b[39mdisplay.width\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m100\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/iangill/Documents/Coding/DataSci/Week2/Descriptive%20statistics.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m#pd.set_option('precision', 3)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/iangill/Documents/Coding/DataSci/Week2/Descriptive%20statistics.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m pd\u001b[39m.\u001b[39mset_option(\u001b[39m'\u001b[39m\u001b[39mprecision\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m3\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/iangill/Documents/Coding/DataSci/Week2/Descriptive%20statistics.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m description \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mdescribe()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/iangill/Documents/Coding/DataSci/Week2/Descriptive%20statistics.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m description\n",
      "File \u001b[0;32m~/Documents/Coding/DataSci/.conda/lib/python3.12/site-packages/pandas/_config/config.py:274\u001b[0m, in \u001b[0;36mCallableDynamicDoc.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[0;32m--> 274\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__func__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[0;32m~/Documents/Coding/DataSci/.conda/lib/python3.12/site-packages/pandas/_config/config.py:167\u001b[0m, in \u001b[0;36m_set_option\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m_set_option() got an unexpected keyword argument \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mkwarg\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    166\u001b[0m \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(args[::\u001b[39m2\u001b[39m], args[\u001b[39m1\u001b[39m::\u001b[39m2\u001b[39m]):\n\u001b[0;32m--> 167\u001b[0m     key \u001b[39m=\u001b[39m _get_single_key(k, silent)\n\u001b[1;32m    169\u001b[0m     o \u001b[39m=\u001b[39m _get_registered_option(key)\n\u001b[1;32m    170\u001b[0m     \u001b[39mif\u001b[39;00m o \u001b[39mand\u001b[39;00m o\u001b[39m.\u001b[39mvalidator:\n",
      "File \u001b[0;32m~/Documents/Coding/DataSci/.conda/lib/python3.12/site-packages/pandas/_config/config.py:134\u001b[0m, in \u001b[0;36m_get_single_key\u001b[0;34m(pat, silent)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[39mraise\u001b[39;00m OptionError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo such keys(s): \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(pat)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    133\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(keys) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 134\u001b[0m     \u001b[39mraise\u001b[39;00m OptionError(\u001b[39m\"\u001b[39m\u001b[39mPattern matched multiple keys\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    135\u001b[0m key \u001b[39m=\u001b[39m keys[\u001b[39m0\u001b[39m]\n\u001b[1;32m    137\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m silent:\n",
      "\u001b[0;31mOptionError\u001b[0m: Pattern matched multiple keys"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.width', 100)\n",
    "#pd.set_option('precision', 3)\n",
    "pd.set_option('precision',3)\n",
    "\n",
    "description = data.describe()\n",
    "\n",
    "description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that you do get a lot of data. You will note some calls to `pandas.set_option()` in the recipe to change the precision of the numbers and the preferred width of the output. This is to make it more readable for this example. When describing your data this way, it is worth taking some time and reviewing observations from the results. This might include the presence of *NA* values for missing data or surprising distributions for attributes.\n",
    "\n",
    "For more information on `pandas.set_option()` please see the <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.set_option.html\">API documentation</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Distribution (Classification only)\n",
    "On classification problems you need to know how balanced the class values are. Highly imbalanced problems (a lot more observations for one class than another) are common and may need special handling in the data preparation stage of your project. \n",
    "\n",
    "You can quickly get an idea of the distribution of the class attribute in Pandas as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = data.groupby('Class').size()\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that there are nearly double the number of observations with class 0 (no onset of \n",
    "diabetes) than there are with class 1 (onset of diabetes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations between Attributes\n",
    "\n",
    "Correlation refers to the relationship between two variables and how they may or may not change together. The most common method for calculating correlation is *Pearson’s Correlation Coefficient*, which assumes a normal distribution of the attributes involved. A correlation of -1 or 1 shows a full negative or positive correlation respectively. Whereas a value of 0 shows no correlation at all. \n",
    "\n",
    "Some machine learning algorithms like linear and logistic regression can suffer poor performance if there are highly correlated attributes in your dataset. As such, it is a good idea to review all of the pairwise correlations of the attributes in your dataset. You can use the `corr()` function on the Pandas `DataFrame` (a multi-dimensional array where the rows and the columns \n",
    "can be labelled) to calculate a correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = data.corr(method='pearson')\n",
    "\n",
    "correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix lists all attributes across the top and down the side, to give correlation between all pairs \n",
    "of attributes (twice, because the matrix is symmetrical). You can see the diagonal line through the \n",
    "matrix from the top left to bottom right corners of the matrix shows perfect correlation of each \n",
    "attribute with itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skew of Univariate Distributions\n",
    "\n",
    "Skew refers to a distribution that is assumed Gaussian (normal or bell curve) that is shifted or \n",
    "squashed in one direction or another. Many machine learning algorithms assume a Gaussian \n",
    "distribution. Knowing that an attribute has a skew may allow you to perform data preparation to \n",
    "correct the skew and later improve the accuracy of your models. You can calculate the skew of each \n",
    "attribute using the `skew()` function on the Pandas `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skew = data.skew()\n",
    "\n",
    "skew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The skew result show a positive (right) or negative (left) skew. Values closer to zero show less \n",
    "skew."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Visualisation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must understand your data in order to get the best results from machine learning algorithms. \n",
    "\n",
    "The fastest way to learn more about your data is to use data visualisation. In this lab you will discover exactly how you can visualise your machine learning data in Python using Pandas. We will continue to use the Pima Indians onset of diabetes dataset introduced in the previous lab. \n",
    "\n",
    "## 2.1 Univariate Plots\n",
    "\n",
    "We will look at three techniques that you can use to understand each attribute of your dataset independently.\n",
    "\n",
    "- Histograms\n",
    "- Density Plots\n",
    "- Box and Whisker Plots\n",
    "\n",
    "### Histograms. \n",
    "\n",
    "A fast way to get an idea of the distribution of each attribute is to look at histograms. Histograms group data into bins and provide you a count of the number of observations in each bin. From the shape of the bins you can quickly get a feeling for whether an attribute is Gaussian, skewed or even has an exponential distribution. It can also help you see possible outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate Histograms\n",
    "data.hist(figsize=[20, 20])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Density Plots. \n",
    "\n",
    "Density plots are another way of getting a quick idea of the distribution of each attribute. The plots look like an abstracted histogram with a smooth curve drawn through the top of each bin, much like your eye tried to do with the histograms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate Density Plots\n",
    "data.plot(kind='density', subplots=True, layout=(3,3), sharex=False, figsize=[20, 20]) \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the above code block and you will be able to see the distribution for each attribute is clearer \n",
    "than the histograms.\n",
    "\n",
    "More details on ```pandas.DataFrame.plot``` function are available in the <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.html\">API documentation</a>. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box and Whisker Plots. \n",
    "\n",
    "Another useful way to review the distribution of each attribute is to use Box and Whisker Plots or boxplots for short. Boxplots summarise the distribution of each attribute, drawing a line for the median (middle value) and a box around the 25th and 75th percentiles (the middle 50% of the data). \n",
    "\n",
    "The whiskers give an idea of the spread of the data and dots outside of the whiskers show candidate outlier values (values that are 1.5 times greater than the size of spread of the middle 50% of the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box and Whisker Plots\n",
    "data.plot(kind='box', subplots=True, layout=(3,3), sharex=False, sharey=False,figsize=[20, 20]) \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the above code block and you will be able to see that\tthe\tspread of attributes is\tquite different. Some like age, test and skin appear quite skewed towards smaller values.\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Multivariate Plots\n",
    "We will look at the examples of two plots that show the interactions between multiple variables in your dataset.\n",
    "\n",
    "- Correlation Matrix Plot.\n",
    "- Scatter Plot Matrix.\n",
    "\n",
    "### Correlation Matrix Plot. \n",
    "\n",
    "Correlation gives an indication of how related the changes are between two variables. If two variables change in the same direction they are positively correlated. If they \n",
    "change in opposite directions together (one goes up, one goes down), then they are negatively correlated. You can calculate the correlation between each pair of attributes. This is called a \n",
    "correlation matrix. You can then plot the correlation matrix and get an idea of which variables have a high correlation with each other. This is useful to know, because some machine learning \n",
    "algorithms like linear and logistic regression can have poor performance if there are highly correlated input variables in your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Matrix Plot\n",
    "correlations = data.corr()\n",
    "\n",
    "# Plot correlation matrix\n",
    "fig = pyplot.figure(figsize=[10, 10])\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(correlations, vmin=-1, vmax=1)\n",
    "fig.colorbar(cax)\n",
    "\n",
    "ticks = np.arange(0,9,1)\n",
    "\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_yticks(ticks)\n",
    "\n",
    "short_names = ['#Preg','Gluco','BloodP','Skin_Th','Insulin','BMI','DPF','Age','Class']\n",
    "\n",
    "ax.set_xticklabels(short_names)\n",
    "ax.set_yticklabels(short_names)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ticks are the values used to show specific points on the coordinate axis. It can be a number or a string. Whenever we plot a graph, the axes adjust and take the default ticks. Matplotlib’s default ticks are generally sufficient in common situations but are in no way optimal for every plot.\n",
    "\n",
    "Run the above code block and you will be able to see that the matrix is symmetrical, i.e. the bottom left of the matrix is the same as the top right. This is useful as we can see two different views on the same data in one plot. We can also see that each variable is perfectly positively correlated with each other (as you would have expected) in the diagonal line from top left to bottom right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above example is not generic in that it specifies the names for the attributes along the axes as well as the number of ticks. We can make the plot more generic by removing the names like the one below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Matrix Plot (generic)\n",
    "\n",
    "# Plot correlation matrix\n",
    "fig = pyplot.figure(figsize=[10, 10])\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(correlations, vmin=-1, vmax=1)\n",
    "fig.colorbar(cax)\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the plot, you can see that it gives the same information although making it a little harder to see what attributes are correlated by name. Use this generic plot as a first cut to understand the \n",
    "correlations in your dataset and customise it like the first example in order to read off more specific data if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter Plot Matrix. \n",
    "\n",
    "A scatter plot shows the relationship between two variables as dots in two dimensions, one axis for each attribute. You can create a scatter plot for each pair of attributes in your data. \n",
    "\n",
    "Drawing all these scatter plots together is called a scatter plot matrix. Scatter plots are useful for spotting structured relationships between variables, like whether you could summarise the relationship between two variables with a line. Attributes with structured relationships may also be correlated and good candidates for removal from your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot Matrix using Pandas\n",
    "\n",
    "pd.plotting.scatter_matrix(data, figsize=[20, 20])\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the Correlation Matrix Plot above, the scatter plot matrix is symmetrical. This is useful to look at the pairwise relationships from different perspectives. Because there is little point of drawing a scatter plot of each variable with itself, the diagonal shows histograms of each attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You almost always need to pre-process your data. It is a required step. A difficulty is that different algorithms make different assumptions about your data and may require different transforms. \n",
    "Furthermore, when you follow all of the rules and prepare your data, sometimes algorithms can deliver better results without pre-processing.\n",
    "\n",
    "Generally, I would recommend creating many different views and transforms of your data, then exercise a handful of algorithms on each view of your dataset. This will help you to flush out which data transforms might be better at exposing the structure of your problem in general.\n",
    "\n",
    "## Data Transforms\n",
    "\n",
    "The scikit-learn library provides two standard idioms for transforming data. Each are useful in different circumstances. The transforms are calculated in such a way that they can be applied to \n",
    "your training data and any samples of data you may have in the future. The scikit-learn documentation has some information on how to use various different pre-processing methods:\n",
    "\n",
    "- Fit and Multiple Transform.\n",
    "- Combined Fit-And-Transform.\n",
    "\n",
    "\n",
    "The Fit and Multiple Transform method is the preferred approach. You call the ```fit()``` function to prepare the parameters of the transform once on your data. Then later you can use the ```transform()``` function on the same data to prepare it for modelling and again on the test or validation dataset or new data that you may see in the future. The Combined Fit-And-Transform is a convenience that you can use for one off tasks. \n",
    "\n",
    "This might be useful if you are interested in plotting or summarising the transformed data. You can review the preprocessing <a href=\"https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing\">API documentation</a> for more information.\n",
    "\n",
    "\n",
    "## Rescale Data\n",
    "\n",
    "When your data is comprised of attributes with varying scales, many machine learning algorithms can benefit from rescaling the attributes to all have the same scale. Often this is referred to as normalisation and attributes are often rescaled into the range between 0 and 1. \n",
    "\n",
    "This is useful for optimisation algorithms used in the core of machine learning algorithms like gradient descent (more on this later). \n",
    "\n",
    "It is also useful for algorithms that weight inputs like regression and neural networks and algorithms that use distance measures like k-Nearest Neighbors (again we will discuss these in more detail). You can rescale your data with scikit-learn using the ```MinMaxScaler``` class. \n",
    "\n",
    "More details are available in the <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\">API documentation</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for rescaling and some of the other transforms we will perform later.\n",
    "\n",
    "# We have loaded the data previously, so now we extract the values from the Pandas dataframe we set up at the beginning.\n",
    "array = data.values\n",
    "\n",
    "# separate array into input and output components\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "print(array)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale data (between 0 and 1)\n",
    "from numpy import set_printoptions\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "rescaledX = scaler.fit_transform(X)\n",
    "\n",
    "# summarise transformed data\n",
    "set_printoptions(precision=3)\n",
    "\n",
    "rescaledX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the above code block and you should be able to see that all of the values are in the range \n",
    "between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardise Data. \n",
    "\n",
    "Standardisation is a useful technique to transform attributes with a Gaussian distribution and differing means and standard deviations to a standard Gaussian distribution with a \n",
    "mean of 0 and a standard deviation of 1. \n",
    "\n",
    "It is most suitable for techniques that assume a Gaussian distribution in the input variables and work better with rescaled data, such as linear regression, \n",
    "logistic regression and linear discriminate analysis. \n",
    "\n",
    "You can standardise data using scikit-learn with the ```StandardScaler``` class. More details are available in the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\">API documentation</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardise data (0 mean, 1 stdev)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(X)\n",
    "rescaledX = scaler.transform(X)\n",
    "\n",
    "# summarise transformed data\n",
    "set_printoptions(precision=3)\n",
    "print(rescaledX[0:5,:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the above code block and you will be able to see that the values for each attribute now have a mean value of 0 and a standard deviation of 1. Now if we plot the original data again...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check to see what the affect was on our attributes after rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original raw data for Pregnancy count\n",
    "pyplot.plot(data[['Pregnancy_Count']].values)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescaled data for Pregnancy count\n",
    "pyplot.plot(rescaledX[:,0])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalise Data\n",
    "\n",
    "Normalising in scikit-learn refers to rescaling each observation (row) to have a length of 1 (called a unit norm or a vector with the length of 1 in linear algebra). This pre-processing method can be useful for sparse datasets (lots of zeros) with attributes of varying scales when using algorithms that weight input values such as neural networks and algorithms that use \n",
    "distance measures such as k-Nearest Neighbors. \n",
    "\n",
    "You can normalise data in Python with scikit-learn using the ```Normalizer``` class. Details are available in the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html\">API documentation</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer \n",
    "# Normalise data (length of 1)\n",
    "scaler = Normalizer().fit(X)\n",
    "\n",
    "normalisedX = scaler.transform(X)\n",
    "\n",
    "# Summarise transformed data\n",
    "set_printoptions(precision=3)\n",
    "print(normalisedX[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarise Data\n",
    "\n",
    "You can transform your data using a binary threshold. All values above the threshold are marked 1 and all equal to or below are marked as 0. This is called binarising your data \n",
    "or thresholding your data. It can be useful when you have probabilities that you want to make crisp values. It is also useful when feature engineering and you want to add new features that indicate \n",
    "something meaningful. \n",
    "\n",
    "You can create new binary attributes in Python using scikit-learn with the ```Binarizer``` class. Details are available in the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Binarizer.html\">API documentation</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binarisation\n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "binariser = Binarizer(threshold=0.0).fit(X)\n",
    "\n",
    "binaryX = binariser.transform(X)\n",
    "\n",
    "# summarise transformed data\n",
    "set_printoptions(precision=3)\n",
    "print(binaryX[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the above code block and you will be able to see that all values equal or less than 0 are marked 0 and all of those above 0 are marked 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Influence of Data transformation on ML models\n",
    "\n",
    "Let's see what the effects of data transformation are on a common Machine Learning algorithm - the DecisionTree classifier. Do not worry too much at this stage if you do not understand the code, we are just interested in the accuracy of the model, before and after data transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree classification \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Normalise data (length of 1)\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "scaler = Normalizer().fit(X)\n",
    "normalisedX = scaler.transform(X)\n",
    "\n",
    "# summarise transformed data\n",
    "set_printoptions(precision=3)\n",
    "\n",
    "print(\"Normalised Samples \\n\", normalisedX[0:5,:])\n",
    "\n",
    "kfold = KFold(n_splits=10)\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "results1 = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(\"\\nMean estimated accuracy \\n\", results1.mean())\n",
    "\n",
    "# Decision tree classification on normalised data\n",
    "results2 = cross_val_score(model, normalisedX, Y, cv=kfold)\n",
    "print(\"\\nMean estimated accuracy on normalised data \\n\",results2.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we initially run our algorithm we achieve a high mean accuracy of 0.69, whereas after we transform and normalise our data we have a much lower, which indicates that transforming and normalising our data had a negative impact on the model performance. In this case, we may try another form of transform or use our data as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
